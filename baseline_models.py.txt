import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# -------------------------------
# Baseline Model Definitions
# -------------------------------

class MobileNetV2_1D(nn.Module):
    def __init__(self):
        super().__init__()
        self.block = nn.Sequential(
            nn.Conv1d(1, 32, kernel_size=1),
            nn.ReLU(),
            nn.Conv1d(32, 32, kernel_size=3, padding=1, groups=32),
            nn.ReLU(),
            nn.Conv1d(32, 64, kernel_size=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1),
            nn.Flatten(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
    def forward(self, x): return self.block(x), None, None, None, None

class LSTMModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(input_size=1, hidden_size=32, batch_first=True)
        self.fc = nn.Sequential(nn.Linear(32, 1), nn.Sigmoid())
    def forward(self, x):
        x = x.transpose(1, 2)
        _, (hn, _) = self.lstm(x)
        return self.fc(hn.squeeze(0)), None, None, None, None

class CNN_LSTM(nn.Module):
    def __init__(self):
        super().__init__()
        self.cnn = nn.Sequential(nn.Conv1d(1, 32, kernel_size=3, padding=1), nn.ReLU())
        self.lstm = nn.LSTM(input_size=32, hidden_size=32, batch_first=True)
        self.fc = nn.Sequential(nn.Linear(32, 1), nn.Sigmoid())
    def forward(self, x):
        x = self.cnn(x).transpose(1, 2)
        _, (hn, _) = self.lstm(x)
        return self.fc(hn.squeeze(0)), None, None, None, None

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv1d(1, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(32 * 5, 1),
            nn.Sigmoid()
        )
    def forward(self, x): return self.net(x), None, None, None, None

class SimpleMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Flatten(),
            nn.Linear(5, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )
    def forward(self, x): return self.net(x), None, None, None, None

class TinyTransformer(nn.Module):
    def __init__(self):
        super().__init__()
        self.embedding = nn.Linear(1, 32)
        encoder_layer = nn.TransformerEncoderLayer(d_model=32, nhead=4, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)
        self.fc = nn.Linear(32, 1)
        self.activation = nn.Sigmoid()

    def forward(self, x):
        x = x.squeeze(1).unsqueeze(-1)
        x = self.embedding(x)
        x = self.transformer(x)
        x = x.mean(dim=1)
        x = self.fc(x)
        x = self.activation(x)
        return x, None, None, None, None

# -------------------------------
# Unified Training Function
# -------------------------------

def train_model(model, train_loader, epochs=20):
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.BCELoss()
    for epoch in range(epochs):
        total_loss = 0
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            preds, *_ = model(batch_x)
            loss = criterion(preds, batch_y)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"{type(model).__name__} | Epoch {epoch+1} | Loss: {total_loss / len(train_loader):.4f}")

# -------------------------------
# Unified Evaluation Function
# -------------------------------

def evaluate_model(model, X, y_true_tensor):
    print(f"ðŸ” Starting evaluation for {type(model).__name__}")
    model.eval()
    with torch.no_grad():
        y_pred_tensor, *_ = model(X)
        y_pred = y_pred_tensor.cpu().numpy().flatten()
        y_true = y_true_tensor.cpu().numpy().flatten()

    y_pred_binary = (y_pred > 0.5).astype(int)
    acc = accuracy_score(y_true, y_pred_binary)
    f1 = f1_score(y_true, y_pred_binary)
    auc = roc_auc_score(y_true, y_pred)

    print(f"âœ… Accuracy: {acc:.4f} | F1: {f1:.4f} | AUC: {auc:.4f}")

    cm = confusion_matrix(y_true, y_pred_binary)
    fig, ax = plt.subplots(figsize=(5, 5))
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap='Blues', ax=ax, values_format='d')
    plt.title(f"Confusion Matrix - {type(model).__name__}")
    plt.grid(False)
    plt.tight_layout()
    plt.show()

    return acc, f1, auc

# -------------------------------
# Run All Baseline Models
# -------------------------------

def run_baseline_models(X_test, y_test, train_loader):
    models = [
        MobileNetV2_1D(),
        LSTMModel(),
        CNN_LSTM(),
        SimpleCNN(),
        SimpleMLP(),
        TinyTransformer()
    ]
    results = {}

    for model in models:
        print(f"\nðŸš€ Training {type(model).__name__}")
        train_model(model, train_loader)
        print(f"ðŸ“Š Evaluating {type(model).__name__}")
        acc, f1, auc = evaluate_model(model, X_test, y_test)
        results[type(model).__name__] = [acc, f1, auc]

    return results
